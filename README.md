# Chatbot Tienda Alemana â€“ Backend (FastAPI)

Este proyecto implementa un backend en **FastAPI** que expone un servicio inteligente de soporte al cliente para la Tienda Alemana. El sistema estÃ¡ basado en la arquitectura **RAG (Retrieval-Augmented Generation)** y permite responder preguntas frecuentes como ubicaciÃ³n de productos, precios y horarios de sucursales.

Internamente, utiliza:

* **LangChain** para orquestar el pipeline.
* **FAISS** como Ã­ndice vectorial.
* **HuggingFace Embeddings** para representaciÃ³n semÃ¡ntica.
* **Ollama** con el modelo local `mistral` para generar respuestas.

> âš ï¸ Requiere Python 3.10 o superior.

## ðŸ“ Estructura del proyecto

```
app/
â”œâ”€â”€ main.py          # Punto de entrada del servidor
â”œâ”€â”€ rag_pipeline.py  # LÃ³gica del pipeline RAG
â”œâ”€â”€ models.py        # Esquemas Pydantic para input/output
â”œâ”€â”€ config.py        # ConfiguraciÃ³n centralizada
vector_store/        # Ãndice FAISS (debe generarse desde notebooks)
requirements.txt     # Lista de dependencias
```

## âš™ï¸ CÃ³mo ejecutar

1. Clonar este repositorio.
2. Crear y activar un entorno virtual.
3. Instalar dependencias:

   ```bash
   pip install -r requirements.txt
   ```
4. Generar el Ã­ndice FAISS ejecutando los notebooks en el directorio `notebooks/`.
   El archivo generado (`vector_store/`) debe copiarse a la raÃ­z del backend.
5. Iniciar el modelo en Ollama:

   ```bash
   ollama run mistral
   ```
6. Levantar el servidor FastAPI:

   ```bash
   uvicorn app.main:app --reload
   ```

## ðŸ“¡ API disponible

### `POST /ask`

* **Request JSON**

```json
{ "question": "Â¿DÃ³nde estÃ¡ el arroz de 1kg?" }
```

* **Response JSON**

```json
{ "answer": "El arroz de 1kg se encuentra en la gÃ³ndola 5, secciÃ³n AlmacÃ©n." }
```

## ðŸ§ª Ejemplo con `curl`

```bash
curl -X POST http://localhost:8000/ask \
     -H "Content-Type: application/json" \
     -d '{"question": "Â¿QuÃ© productos hay en la gÃ³ndola 2?"}'
```

---

> ðŸ“Œ **Importante:** El backend no funcionarÃ¡ si no se incluye el directorio `vector_store/` generado desde los notebooks, ni si el modelo `mistral` no estÃ¡ corriendo en Ollama.

---

## ðŸ“¦ requirements.txt

```txt
fastapi
uvicorn
langchain
langchain-community
langchain-huggingface
langchain-ollama
sentence-transformers
faiss-cpu
pydantic
tiktoken
```
